[{"authors":null,"categories":null,"content":"I am a first year PhD student in the Data Analytics Lab at ETH Zurich. Currently, I\u0026rsquo;m working on natural language processing and non-convex optimization.\nPrior to this, I obtained a Master’s degree from ETH in Computer Science and a Bachelor’s degree from KIT Karlsruhe in electrical engineering.\nA copy of my CV can be found here.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://leox1v.com/author/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/admin/","section":"author","summary":"I am a first year PhD student in the Data Analytics Lab at ETH Zurich. Currently, I\u0026rsquo;m working on natural language processing and non-convex optimization.\nPrior to this, I obtained a Master’s degree from ETH in Computer Science and a Bachelor’s degree from KIT Karlsruhe in electrical engineering.\nA copy of my CV can be found here.","tags":null,"title":"","type":"author"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d41d8cd98f00b204e9800998ecf8427e","permalink":"https://leox1v.com/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"author","summary":"","tags":null,"title":"Authors","type":"author"},{"authors":["**L Adolphs***","H Daneshmand","A Lucchi","T Hofmann"],"categories":null,"content":"","date":1526342400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526342400,"objectID":"cb48775ac77b2e92d4c6d24cbdf9628b","permalink":"https://leox1v.com/publication/local_saddle_opt/","publishdate":"2018-05-15T00:00:00Z","relpermalink":"/publication/local_saddle_opt/","section":"publication","summary":"Gradient-based optimization methods are the most popular choice for ﬁnding local optima for classical minimization and saddle point problems. Here, we highlight a systemic issue of gradient dynamics that arise for saddle point problems, namely the presence of undesired stable stationary points that are no local optima. We propose a novel optimization approach that exploits curvature information in order to escape from these undesired stationary points. We prove that diﬀerent optimization methods, including gradient method and Adagrad, equipped with curvature exploitation can escape non-optimal stationary points. We also provide empirical results on common saddle point problems which conﬁrm the advantage of using curvature exploitation.","tags":[],"title":"Local Saddle Point Optimization: A Curvature Exploitation Approach","type":"publication"},{"authors":["**L Adolphs** *","J Kohler *","A Lucchi"],"categories":null,"content":"","date":1516665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516665600,"objectID":"7e3c8df403b9c3b7442e958795009123","permalink":"https://leox1v.com/publication/etr/","publishdate":"2018-01-23T00:00:00Z","relpermalink":"/publication/etr/","section":"publication","summary":"We investigate the use of ellipsoidal trust region constraints for second-order optimization of neural networks. This approach can be seen as a higher-order counterpart of adaptive gradient methods, which we here show to be interpretable as first-order trust region methods with ellipsoidal constraints. In particular, we show that the preconditioning matrix used in RMSProp and Adam satisfies the necessary conditions for convergence of (first- and) second-order trust region methods and report that this ellipsoidal constraint constantly outperforms its spherical counterpart in practice. We furthermore set out to clarify the long-standing question of the potential superiority of Newton methods in deep learning. In this regard, we run extensive benchmarks across different datasets and architectures to find that comparable performance to gradient descent algorithms can be achieved but using Hessian information does not give rise to better limit points and comes at the cost of increased hyperparameter tuning.","tags":[],"title":"Ellipsoidal Trust Region Methods and the Marginal Value of Hessian Information for Neural Network Training","type":"publication"}]
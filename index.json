[{"authors":null,"categories":null,"content":"I am a second year PhD student in the Data Analytics Lab at ETH Zurich. Currently, I\u0026rsquo;m working on natural language processing and non-convex optimization.\nPrior to this, I obtained a Master‚Äôs degree from ETH in Computer Science and a Bachelor‚Äôs degree from KIT Karlsruhe in electrical engineering.\nA copy of my CV can be found here.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://leox1v.com/author/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/admin/","section":"author","summary":"I am a second year PhD student in the Data Analytics Lab at ETH Zurich. Currently, I\u0026rsquo;m working on natural language processing and non-convex optimization.\nPrior to this, I obtained a Master‚Äôs degree from ETH in Computer Science and a Bachelor‚Äôs degree from KIT Karlsruhe in electrical engineering.\nA copy of my CV can be found here.","tags":null,"title":"","type":"author"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d41d8cd98f00b204e9800998ecf8427e","permalink":"https://leox1v.com/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"author","summary":"","tags":null,"title":"Authors","type":"author"},{"authors":["**L Adolphs***","T Hofmann"],"categories":null,"content":"","date":1566950400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566950400,"objectID":"5a387d24001b80d72516e89545b9c988","permalink":"https://leox1v.com/publication/ftwp/","publishdate":"2019-08-28T00:00:00Z","relpermalink":"/publication/ftwp/","section":"publication","summary":"While Reinforcement Learning (RL) approaches lead to significant achievements in a variety of areas in recent history, natural language tasks remained mostly unaffected, due to the compositional and combinatorial nature that makes them notoriously hard to optimize. With the emerging field of Text-Based Games (TBGs), researchers try to bridge this gap. Inspired by the success of RL algorithms on Atari games, the idea is to develop new methods in a restricted game world and then gradually move to more complex environments. Previous work in the area of TBGs has mainly focused on solving individual games. We, however, consider the task of designing an agent that not just succeeds in a single game, but performs well across a whole family of games, sharing the same theme. In this work, we present our deep RL agent---LeDeepChef---that shows generalization capabilities to never-before-seen games of the same family with different environments and task descriptions. The agent participated in Microsoft Research's *First TextWorld Problems: A Language and Reinforcement Learning Challenge* and outperformed all but one competitor on the final test set. The games from the challenge all share the same theme, namely cooking in a modern house environment, but differ significantly in the arrangement of the rooms, the presented objects, and the specific goal (recipe to cook). To build an agent that achieves high scores across a whole family of games, we use an actor-critic framework and prune the action-space by using ideas from hierarchical reinforcement learning and a specialized module trained on a recipe database.","tags":[],"title":"LeDeepChef üë®üèª‚Äçüç≥ : Deep Reinforcement Learning Agent for Families of Text-Based Games","type":"publication"},{"authors":["**L Adolphs***","J Kohler *","A Lucchi"],"categories":null,"content":"","date":1548201600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548201600,"objectID":"7e3c8df403b9c3b7442e958795009123","permalink":"https://leox1v.com/publication/etr/","publishdate":"2019-01-23T00:00:00Z","relpermalink":"/publication/etr/","section":"publication","summary":"We investigate the use of ellipsoidal trust region constraints for second-order optimization of neural networks. This approach can be seen as a higher-order counterpart of adaptive gradient methods, which we here show to be interpretable as first-order trust region methods with ellipsoidal constraints. In particular, we show that the preconditioning matrix used in RMSProp and Adam satisfies the necessary conditions for convergence of (first- and) second-order trust region methods and report that this ellipsoidal constraint constantly outperforms its spherical counterpart in practice. We furthermore set out to clarify the long-standing question of the potential superiority of Newton methods in deep learning. In this regard, we run extensive benchmarks across different datasets and architectures to find that comparable performance to gradient descent algorithms can be achieved but using Hessian information does not give rise to better limit points and comes at the cost of increased hyperparameter tuning.","tags":[],"title":"Ellipsoidal Trust Region Methods and the Marginal Value of Hessian Information for Neural Network Training","type":"publication"},{"authors":["**L Adolphs***","H Daneshmand","A Lucchi","T Hofmann"],"categories":null,"content":"","date":1526342400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526342400,"objectID":"cb48775ac77b2e92d4c6d24cbdf9628b","permalink":"https://leox1v.com/publication/local_saddle_opt/","publishdate":"2018-05-15T00:00:00Z","relpermalink":"/publication/local_saddle_opt/","section":"publication","summary":"Gradient-based optimization methods are the most popular choice for Ô¨Ånding local optima for classical minimization and saddle point problems. Here, we highlight a systemic issue of gradient dynamics that arise for saddle point problems, namely the presence of undesired stable stationary points that are no local optima. We propose a novel optimization approach that exploits curvature information in order to escape from these undesired stationary points. We prove that diÔ¨Äerent optimization methods, including gradient method and Adagrad, equipped with curvature exploitation can escape non-optimal stationary points. We also provide empirical results on common saddle point problems which conÔ¨Årm the advantage of using curvature exploitation.","tags":[],"title":"Local Saddle Point Optimization: A Curvature Exploitation Approach","type":"publication"}]
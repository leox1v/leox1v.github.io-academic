[{"authors":null,"categories":null,"content":"I am a first year PhD student in the Data Analytics Lab at ETH Zurich. Currently, I\u0026rsquo;m working on natural language processing and non-convex optimization.\nPrior to this, I obtained a Master’s degree from ETH in Computer Science and a Bachelor’s degree from KIT Karlsruhe in electrical engineering.\nA copy of my CV can be found here.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://leox1v.com/author/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/admin/","section":"author","summary":"I am a first year PhD student in the Data Analytics Lab at ETH Zurich. Currently, I\u0026rsquo;m working on natural language processing and non-convex optimization.\nPrior to this, I obtained a Master’s degree from ETH in Computer Science and a Bachelor’s degree from KIT Karlsruhe in electrical engineering.\nA copy of my CV can be found here.","tags":null,"title":"","type":"author"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d41d8cd98f00b204e9800998ecf8427e","permalink":"https://leox1v.com/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"author","summary":"","tags":null,"title":"Authors","type":"author"},{"authors":["**L Adolphs***","H Daneshmand","A Lucchi","T Hofmann"],"categories":null,"content":"","date":1526342400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526342400,"objectID":"cb48775ac77b2e92d4c6d24cbdf9628b","permalink":"https://leox1v.com/publication/local_saddle_opt/","publishdate":"2018-05-15T00:00:00Z","relpermalink":"/publication/local_saddle_opt/","section":"publication","summary":"Gradient-based optimization methods are the most popular choice for ﬁnding local optima for classical minimization and saddle point problems. Here, we highlight a systemic issue of gradient dynamics that arise for saddle point problems, namely the presence of undesired stable stationary points that are no local optima. We propose a novel optimization approach that exploits curvature information in order to escape from these undesired stationary points. We prove that diﬀerent optimization methods, including gradient method and Adagrad, equipped with curvature exploitation can escape non-optimal stationary points. We also provide empirical results on common saddle point problems which conﬁrm the advantage of using curvature exploitation.","tags":[],"title":"Local Saddle Point Optimization: A Curvature Exploitation Approach","type":"publication"},{"authors":["J Kohler*","**L Adolphs***","A Lucchi"],"categories":null,"content":"","date":1516665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516665600,"objectID":"7e3c8df403b9c3b7442e958795009123","permalink":"https://leox1v.com/publication/etr/","publishdate":"2018-01-23T00:00:00Z","relpermalink":"/publication/etr/","section":"publication","summary":"We propose the use of ellipsoidal trust region constraints for second-order optimization of neural networks. This approach can be seen as the second-order counterpart of adaptive gradient methods such as Adagrad, which we here show to be interpretable as ﬁrst-order trust region methods with ellipsoidal constraints. We show that these ellipsoids satisfy the necessary conditions for convergence and report that they constantly outperform their spherical counterpart. For large-scale datasets, we propose a modiﬁcation to the trust region framework to improve its computational efﬁciency. In this regard, we report comparable performance to ﬁrst-order algorithms in terms of wall-clock time and often superior performance in terms of backpropagations and epochs. Next to algorithmic advances of second-order methods, this work also provides new insights into the inner workings of adaptive gradient methods. Particularly, we ﬁnd that diagonal preconditioning is very effective since many neural network problems exhibit diagonally dominated Hessian matrices.","tags":[],"title":"Ellipsoidal Trust Region Methods for Neural Network Optimization","type":"publication"}]